{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91a4f64-2568-4b90-8fb2-17f5ccde1c4d",
   "metadata": {},
   "source": [
    "# Comparing Model Performance after Fine-Tuning\n",
    "In this example, we will take the pre-existing SageMaker endpoints that you deployed in previous exercises and use them to generate data that can be leveraged for quality comparison. This data can be used to take a quantitative approach to judge the efficacy of fine-tuning your models.\n",
    "\n",
    "This example will run through samples of the Samsum dataset (paper here) on the Hugging Face data hub to generate summaries of earnings calls transcripts and use the [lighteval](https://huggingface.co/docs/lighteval/index) from Hugging Face for analysis on those summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353628ef-4cf9-4957-85ee-1667ac4de611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.36.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: lighteval[math] in /opt/conda/lib/python3.11/site-packages (0.8.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.23 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.36.23)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: transformers>=4.38.0 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (4.51.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (1.6.0)\n",
      "Requirement already satisfied: torch<3.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (2.4.1.post100)\n",
      "Requirement already satisfied: GitPython>=3.1.41 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (3.1.44)\n",
      "Requirement already satisfied: typer in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (0.15.2)\n",
      "Requirement already satisfied: termcolor==2.3.0 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (2.3.0)\n",
      "Requirement already satisfied: pytablewriter in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (1.2.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (13.9.4)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (6.9.0)\n",
      "Requirement already satisfied: aenum==3.1.15 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (3.1.15)\n",
      "Requirement already satisfied: nltk==3.9.1 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (3.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (1.6.1)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (2.5.1)\n",
      "Requirement already satisfied: rouge_score==0.1.2 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (0.1.2)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (0.2.0)\n",
      "Requirement already satisfied: protobuf==3.20.* in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (3.20.3)\n",
      "Requirement already satisfied: pycountry in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (24.6.1)\n",
      "Requirement already satisfied: latex2sympy2_extended==1.0.6 in /opt/conda/lib/python3.11/site-packages (from lighteval[math]) (1.0.6)\n",
      "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended==1.0.6->lighteval[math])\n",
      "  Using cached antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from latex2sympy2_extended==1.0.6->lighteval[math]) (1.13.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk==3.9.1->lighteval[math]) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk==3.9.1->lighteval[math]) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk==3.9.1->lighteval[math]) (2024.11.6)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge_score==0.1.2->lighteval[math]) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge_score==0.1.2->lighteval[math]) (1.17.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.37.0,>=1.36.23->boto3) (2.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from GitPython>=3.1.41->lighteval[math]) (4.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch<3.0,>=2.0->lighteval[math]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<3.0,>=2.0->lighteval[math]) (3.1.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.0->lighteval[math]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.38.0->lighteval[math]) (0.5.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->lighteval[math]) (5.9.8)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.11/site-packages (from pytablewriter->lighteval[math]) (75.8.2)\n",
      "Requirement already satisfied: DataProperty<2,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from pytablewriter->lighteval[math]) (1.1.0)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from pytablewriter->lighteval[math]) (1.1.4)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from pytablewriter->lighteval[math]) (3.2.3)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from pytablewriter->lighteval[math]) (1.3.4)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /opt/conda/lib/python3.11/site-packages (from pytablewriter->lighteval[math]) (0.1.7)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /opt/conda/lib/python3.11/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lighteval[math]) (1.3.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->lighteval[math]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->lighteval[math]) (2.19.1)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.11/site-packages (from sacrebleu->lighteval[math]) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.11/site-packages (from sacrebleu->lighteval[math]) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.11/site-packages (from sacrebleu->lighteval[math]) (0.4.6)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from sacrebleu->lighteval[math]) (5.3.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->lighteval[math]) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->lighteval[math]) (3.5.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from typer->lighteval[math]) (1.5.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.41->lighteval[math]) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->lighteval[math]) (0.1.2)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /opt/conda/lib/python3.11/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lighteval[math]) (5.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<3.0,>=2.0->lighteval[math]) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->latex2sympy2_extended==1.0.6->lighteval[math]) (1.3.0)\n",
      "Using cached antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
      "Installing collected packages: antlr4-python3-runtime\n",
      "  Attempting uninstall: antlr4-python3-runtime\n",
      "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
      "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
      "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "omegaconf 2.3.0 requires antlr4-python3-runtime==4.9.*, but you have antlr4-python3-runtime 4.13.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages and restart the kernel\n",
    "%pip install datasets pandas matplotlib numpy boto3 tqdm lighteval[math]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c20c83d-2050-494c-acd6-0c9f575eb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "\n",
    "# Import LightEval metrics\n",
    "from lighteval.metrics.metrics_sample import ROUGE, Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1341fb-a37e-4f9f-9d3f-32233d58427f",
   "metadata": {},
   "source": [
    "#### Update the base model and fine-tuned model endpoints with the names of the endpoints you previously created. \n",
    "You can find these in **SageMaker Studio > Deployments > Endpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821e1176-f2af-4e7f-9273-48b2d67e22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SageMaker client\n",
    "sm_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Configure the SageMaker endpoint names\n",
    "BASE_MODEL_ENDPOINT = \"huggingface-pytorch-tgi-inference-2025-04-06-21-48-45-822\" # Update with Base model endpoint name\n",
    "FINETUNED_MODEL_ENDPOINT = \"DeepSeek-R1-Distill-Llama-8B-finetuned\"  # Update with Fine-tuned model endpoint name\n",
    "\n",
    "# Define the model to evaluate\n",
    "model_to_evaluate = {\n",
    "    \"name\": \"Fine-tuned DeepSeek-R1-Distill-Llama-8B\", \n",
    "    \"endpoint\": FINETUNED_MODEL_ENDPOINT\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de5205-0203-441b-b8d6-e2a8ef6c7fae",
   "metadata": {},
   "source": [
    "Here you will use the the Samsum dataset. The dataset is pre-split into training and test data. We will limit the number of samples to evaluate for the fine-tuned and base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a89eec7-cac2-4295-a0d6-495bb445d267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SAMSum dataset with 10 samples\n",
      "Dialogue:\n",
      " Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye \n",
      "\n",
      "Summary:\n",
      " Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    }
   ],
   "source": [
    "# Dataset to use for evaluation - using SAMSum\n",
    "dataset_name = \"samsum\"\n",
    "split = \"test\"\n",
    "\n",
    "# Limit the number of samples to evaluate (for faster execution)\n",
    "num_samples = 10\n",
    "\n",
    "# Load the test split of the SAMSum dataset\n",
    "dataset = load_dataset(dataset_name, split=split)\n",
    "dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "print(f\"Loaded SAMSum dataset with {len(dataset)} samples\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "sample = dataset[0]\n",
    "print(\"Dialogue:\\n\", sample[\"dialogue\"], \"\\n\")\n",
    "print(\"Summary:\\n\", sample[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a5e2c-39e9-4d51-a394-b666ffde44f2",
   "metadata": {},
   "source": [
    "#### Next, we will create functions to interact with the SageMaker endpoints, define metrics we want to calculate (ROUGE), and define how to evaluate the models with the Samsum dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957d8b1e-0761-4f00-ae6d-dc0c9530e46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows you to interact with a deployed SageMaker endpoint to get predictions from the DeepSeek model\n",
    "def invoke_sagemaker_endpoint(payload, endpoint_name):\n",
    "    \"\"\"\n",
    "    Invoke a SageMaker endpoint with the given payload.\n",
    "    \n",
    "    Args:\n",
    "        payload (dict): The input data to send to the endpoint\n",
    "        endpoint_name (str): The name of the SageMaker endpoint\n",
    "        \n",
    "    Returns:\n",
    "        dict: The response from the endpoint\n",
    "    \"\"\"\n",
    "    response = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    response_body = response['Body'].read().decode('utf-8')\n",
    "    return json.loads(response_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8cd7d2a-ed91-45fa-8f1c-dfe42f9ebc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LightEval metrics calculators\n",
    "rouge_metrics = ROUGE(\n",
    "    methods=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    multiple_golds=False,\n",
    "    bootstrap=False,\n",
    "    normalize_gold=None,\n",
    "    normalize_pred=None\n",
    ")\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate all evaluation metrics for summarization using LightEval.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of generated summaries\n",
    "        references (list): List of reference summaries\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all metric scores\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Create Doc objects for the Rouge and BertScore metrics\n",
    "    docs = []\n",
    "    for reference in references:\n",
    "        docs.append(Doc(\n",
    "            {\"target\": reference},\n",
    "            choices=[reference],  # Dummy choices\n",
    "            gold_index=0  # Dummy gold_index\n",
    "        ))\n",
    "    \n",
    "    # Calculate ROUGE scores for each prediction-reference pair\n",
    "    rouge_scores = {'rouge1_f': [], 'rouge2_f': [], 'rougeL_f': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # For ROUGE calculation\n",
    "        rouge_result = rouge_metrics.compute(golds=[ref], predictions=[pred])\n",
    "        rouge_scores['rouge1_f'].append(rouge_result['rouge1'])\n",
    "        rouge_scores['rouge2_f'].append(rouge_result['rouge2'])\n",
    "        rouge_scores['rougeL_f'].append(rouge_result['rougeL'])\n",
    "    \n",
    "    # Average ROUGE scores\n",
    "    for key in rouge_scores:\n",
    "        metrics[key] = sum(rouge_scores[key]) / len(rouge_scores[key])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccac55f4-463b-4a25-bbbd-529f76fbc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_with_model(endpoint_name, dataset):\n",
    "    \"\"\"\n",
    "    Generate summaries using a model deployed on SageMaker.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): SageMaker endpoint name\n",
    "        dataset: Dataset containing dialogues\n",
    "        \n",
    "    Returns:\n",
    "        list: Generated summaries\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Generating summaries\"):\n",
    "        dialogue = example[\"dialogue\"]\n",
    "        \n",
    "        # Prepare the prompt for the model\n",
    "        prompt = f\"\"\"Please summarize the following conversation concisely:\n",
    "\n",
    "Conversation:\n",
    "{dialogue}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        # Payload for SageMaker endpoint\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call the model endpoint\n",
    "        try:\n",
    "            response = invoke_sagemaker_endpoint(payload, endpoint_name)\n",
    "            \n",
    "            # Extract the generated text\n",
    "            if isinstance(response, list):\n",
    "                prediction = response[0].get('generated_text', '').strip()\n",
    "            elif isinstance(response, dict):\n",
    "                prediction = response.get('generated_text', '').strip()\n",
    "            else:\n",
    "                prediction = str(response).strip\n",
    "            \n",
    "            # Clean up the generated text\n",
    "            if \"Summary:\" in prediction:\n",
    "                prediction = prediction.split(\"Summary:\", 1)[1].strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "            prediction = \"Error generating summary.\"\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a872572-5d5d-4a0e-98f6-e656f26ba29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_samsum(model_config, dataset):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model on the SamSum dataset using both automated and human metrics.\n",
    "    \n",
    "    Args:\n",
    "        model_config (dict): Model configuration with name and endpoint\n",
    "        dataset: SamSum dataset for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    model_name = model_config[\"name\"]\n",
    "    endpoint_name = model_config[\"endpoint\"]\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name} on endpoint: {endpoint_name}\")\n",
    "    \n",
    "    # Get references\n",
    "    references = [example[\"summary\"] for example in dataset]\n",
    "    \n",
    "    # Generate summaries\n",
    "    print(\"\\nGenerating summaries...\")\n",
    "    predictions = generate_summaries_with_model(endpoint_name, dataset)\n",
    "    \n",
    "    # Calculate automated metrics using LightEval\n",
    "    print(\"\\nCalculating evaluation metrics with LightEval...\")\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # Format results\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"num_samples\": len(dataset),\n",
    "        \"metrics\": metrics,\n",
    "        \"predictions\": predictions[:5],  # First 5 predictions\n",
    "        \"references\": references[:5]     # First 5 references\n",
    "    }\n",
    "    \n",
    "    # Print key results\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"ROUGE-1 F1: {metrics['rouge1_f']:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {metrics['rouge2_f']:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {metrics['rougeL_f']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aa020-0aaf-438d-b0cb-dd503d248feb",
   "metadata": {},
   "source": [
    "#### In this section, we evaluate the performance of both our base model (DeepSeek-R1-Distill-Llama-8B) and our fine-tuned model on the SAMSum dataset using ROUGE metrics, which are standard for evaluating text summarization quality.\n",
    "\n",
    "The evaluation process:\n",
    "\n",
    "We first evaluate the base model against the SAMSum test set to establish a baseline performance. Then, we evaluate our fine-tuned model on the same dataset to measure improvements. Both evaluations calculate ROUGE-1, ROUGE-2, and ROUGE-L scores, which respectively measure:\n",
    "\n",
    "ROUGE-1: Unigram overlap between generated and reference summaries\n",
    "ROUGE-2: Bigram overlap (captures more fluency and coherence)\n",
    "ROUGE-L: Longest common subsequence (measures sentence structure similarity)\n",
    "\n",
    "The results are saved to JSON files for later analysis and comparison. These metrics will help us quantify how much our fine-tuning process has improved the model's summarization capabilities compared to the original base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02db13a7-81a3-4279-bde5-afbc75ca2b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Base DeepSeek-R1-Distill-Llama-8B on endpoint: huggingface-pytorch-tgi-inference-2025-04-06-21-48-45-822\n",
      "\n",
      "Generating summaries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f407fe47ad4646daaebd458e5d7e9951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating summaries:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating evaluation metrics with LightEval...\n",
      "\n",
      "Results for Base DeepSeek-R1-Distill-Llama-8B:\n",
      "ROUGE-1 F1: 0.2471\n",
      "ROUGE-2 F1: 0.0646\n",
      "ROUGE-L F1: 0.1683\n",
      "\n",
      "Evaluating model: Fine-tuned DeepSeek-R1-Distill-Llama-8B on endpoint: DeepSeek-R1-Distill-Llama-8B-finetuned\n",
      "\n",
      "Generating summaries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d33bf1e250540fb88f62f2c1f1510be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating summaries:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating evaluation metrics with LightEval...\n",
      "\n",
      "Results for Fine-tuned DeepSeek-R1-Distill-Llama-8B:\n",
      "ROUGE-1 F1: 0.2700\n",
      "ROUGE-2 F1: 0.0998\n",
      "ROUGE-L F1: 0.2030\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the base and fine-tuned models using LightEval metrics\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate both models for comparison\n",
    "base_model_config = {\n",
    "    \"name\": \"Base DeepSeek-R1-Distill-Llama-8B\",\n",
    "    \"endpoint\": BASE_MODEL_ENDPOINT\n",
    "}\n",
    "\n",
    "# Evaluate base model\n",
    "base_model_results = evaluate_model_on_samsum(base_model_config, dataset)\n",
    "base_model_results[\"evaluation_time\"] = time.time() - start_time\n",
    "\n",
    "# Start timing fine-tuned model\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "finetuned_model_results = evaluate_model_on_samsum(model_to_evaluate, dataset)\n",
    "finetuned_model_results[\"evaluation_time\"] = time.time() - start_time\n",
    "\n",
    "# Save results\n",
    "base_file_name = base_model_config[\"name\"].replace(' ', '_').lower()\n",
    "finetuned_file_name = model_to_evaluate[\"name\"].replace(' ', '_').lower()\n",
    "\n",
    "with open(f\"{base_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(base_model_results, f)\n",
    "    \n",
    "with open(f\"{finetuned_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(finetuned_model_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4944d-ad64-448d-91aa-e7222d7ebe8a",
   "metadata": {},
   "source": [
    "Create a tablular view to compare the base model and fine-tuned model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336396df-7175-4069-9794-b425501aee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>ROUGE-1 F1</th>\n",
       "      <th>ROUGE-2 F1</th>\n",
       "      <th>ROUGE-L F1</th>\n",
       "      <th>Evaluation Time (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base DeepSeek-R1-Distill-Llama-8B</td>\n",
       "      <td>0.247079</td>\n",
       "      <td>0.064579</td>\n",
       "      <td>0.168272</td>\n",
       "      <td>35.473946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fine-tuned DeepSeek-R1-Distill-Llama-8B</td>\n",
       "      <td>0.270049</td>\n",
       "      <td>0.099848</td>\n",
       "      <td>0.203041</td>\n",
       "      <td>35.705987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Model  ROUGE-1 F1  ROUGE-2 F1  \\\n",
       "0        Base DeepSeek-R1-Distill-Llama-8B    0.247079    0.064579   \n",
       "1  Fine-tuned DeepSeek-R1-Distill-Llama-8B    0.270049    0.099848   \n",
       "\n",
       "   ROUGE-L F1  Evaluation Time (s)  \n",
       "0    0.168272            35.473946  \n",
       "1    0.203041            35.705987  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Add base model metrics\n",
    "comparison_data.append({\n",
    "    \"Model\": base_model_config[\"name\"],\n",
    "    \"ROUGE-1 F1\": base_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": base_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": base_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": base_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "# Add fine-tuned model metrics\n",
    "comparison_data.append({\n",
    "    \"Model\": model_to_evaluate[\"name\"],\n",
    "    \"ROUGE-1 F1\": finetuned_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": finetuned_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": finetuned_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": finetuned_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07ee65-270a-41ce-bdff-620318c673f6",
   "metadata": {},
   "source": [
    "Show a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "310fa953-173a-433b-ac83-7f75a45907eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot ROUGE and BERTScore metrics for both models\n",
    "metrics_to_plot = [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]\n",
    "models = comparison_df[\"Model\"].tolist()\n",
    "\n",
    "# Create a grouped bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(metrics_to_plot))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    values = [comparison_df.loc[i, metric] for metric in metrics_to_plot]\n",
    "    plt.bar(index + i*bar_width, values, bar_width, label=model)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Summarization Performance Comparison')\n",
    "plt.xticks(index + bar_width/2, metrics_to_plot)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eabf665-2e81-4b17-8ce2-3b7aec06e010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement Analysis:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Base Score</th>\n",
       "      <th>Fine-tuned Score</th>\n",
       "      <th>Absolute Improvement</th>\n",
       "      <th>% Improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROUGE-1 F1</td>\n",
       "      <td>0.247079</td>\n",
       "      <td>0.270049</td>\n",
       "      <td>0.022970</td>\n",
       "      <td>9.30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROUGE-2 F1</td>\n",
       "      <td>0.064579</td>\n",
       "      <td>0.099848</td>\n",
       "      <td>0.035269</td>\n",
       "      <td>54.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROUGE-L F1</td>\n",
       "      <td>0.168272</td>\n",
       "      <td>0.203041</td>\n",
       "      <td>0.034769</td>\n",
       "      <td>20.66%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Metric  Base Score  Fine-tuned Score  Absolute Improvement  \\\n",
       "0  ROUGE-1 F1    0.247079          0.270049              0.022970   \n",
       "1  ROUGE-2 F1    0.064579          0.099848              0.035269   \n",
       "2  ROUGE-L F1    0.168272          0.203041              0.034769   \n",
       "\n",
       "  % Improvement  \n",
       "0         9.30%  \n",
       "1        54.61%  \n",
       "2        20.66%  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate improvement from base to fine-tuned model\n",
    "improvement_data = {}\n",
    "\n",
    "for metric in [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]:\n",
    "    base_value = comparison_df.loc[0, metric]\n",
    "    finetuned_value = comparison_df.loc[1, metric]\n",
    "    \n",
    "    if not pd.isna(base_value) and not pd.isna(finetuned_value):\n",
    "        abs_improvement = finetuned_value - base_value\n",
    "        pct_improvement = (abs_improvement / base_value) * 100 if base_value > 0 else float('inf')\n",
    "        \n",
    "        improvement_data[metric] = {\n",
    "            \"Base Model\": base_value,\n",
    "            \"Fine-tuned Model\": finetuned_value,\n",
    "            \"Absolute Improvement\": abs_improvement,\n",
    "            \"% Improvement\": pct_improvement\n",
    "        }\n",
    "\n",
    "# Create DataFrame for improvement metrics\n",
    "improvement_df = pd.DataFrame({\n",
    "    \"Metric\": list(improvement_data.keys()),\n",
    "    \"Base Score\": [improvement_data[m][\"Base Model\"] for m in improvement_data],\n",
    "    \"Fine-tuned Score\": [improvement_data[m][\"Fine-tuned Model\"] for m in improvement_data],\n",
    "    \"Absolute Improvement\": [improvement_data[m][\"Absolute Improvement\"] for m in improvement_data],\n",
    "    \"% Improvement\": [f\"{improvement_data[m]['% Improvement']:.2f}%\" for m in improvement_data]\n",
    "})\n",
    "\n",
    "print(\"Improvement Analysis:\")\n",
    "improvement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6fd3c6-19a9-41c7-97e2-c03ab2023ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Dialogue (truncated): Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her ...\n",
      "\n",
      "Reference Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "Base Model Summary: [Your concise summary here]\n",
      "---\n",
      "\n",
      "Alright, so I need to summarize this conversation between Hannah and Amanda. Let me read through it carefully to understand what's going on.\n",
      "\n",
      "Hannah starts by asking Amanda if she has Betty's number. Amanda says she'll check. Then Hannah sends a file_gif, which I think might be an emoji or some kind of media file. Amanda responds that she can't find the number, so she suggests asking Larry. She mentions that Larry called Betty last time they\n",
      "\n",
      "Fine-tuned Model Summary: Hannah asks Amanda for Betty's number, Amanda says she can't find it and suggests asking Larry, who called Betty last time they were at the park together. Amanda encourages Hannah to text Larry, and they say bye.\n",
      "\n",
      "**Step-by-step explanation:**\n",
      "\n",
      "1. **Identify the key elements:** The conversation revolves around Hannah asking for Betty's number and Amanda's suggestion to contact Larry.\n",
      "2. **Extract the main actions:** Hannah asks for the number, Amanda checks and can't find it,\n",
      "\n",
      "ROUGE Scores (LightEval):\n",
      "Base Model - ROUGE-1: 0.1961, ROUGE-2: 0.0400, ROUGE-L: 0.1569\n",
      "Fine-tuned - ROUGE-1: 0.2245, ROUGE-2: 0.0833, ROUGE-L: 0.2041\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 2:\n",
      "Dialogue (truncated): Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially lik...\n",
      "\n",
      "Reference Summary: Eric and Rob are going to watch a stand-up on youtube.\n",
      "\n",
      "Base Model Summary: Okay, so I need to summarize this conversation between Eric and Rob. Let me read through it again. They start with Eric saying \"MACHINE!\", and Rob responds with \"That's so gr8!\" It seems like they're talking about something funny or interesting they saw, probably a video or a stand-up act. Eric mentions liking the train part and Rob agrees, finding it funny. They talk about how no one would talk to a machine like that, which makes me think it's a\n",
      "\n",
      "Fine-tuned Model Summary: Okay, so I need to summarize this conversation between Eric and Rob. Let me read through it again to make sure I understand what they're talking about.\n",
      "\n",
      "Eric starts by saying \"MACHINE\" and Rob responds with \"That's so gr8!\" which I think means \"That's so great!\" or \"That's so cool!\" Then Eric says he knows and that it shows how Americans see Russians. Rob agrees it's funny. Eric mentions liking the train part, and Rob laughs, saying\n",
      "\n",
      "ROUGE Scores (LightEval):\n",
      "Base Model - ROUGE-1: 0.1443, ROUGE-2: 0.0842, ROUGE-L: 0.1237\n",
      "Fine-tuned - ROUGE-1: 0.0851, ROUGE-2: 0.0435, ROUGE-L: 0.0851\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Example 3:\n",
      "Dialogue (truncated): Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "Bob: Send me photos\n",
      "Lenny:  <file_photo>\n",
      "Lenny...\n",
      "\n",
      "Reference Summary: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
      "\n",
      "Base Model Summary: Lenny is deciding between multiple pairs of trousers. Bob advises him to send photos, then based on that, Bob suggests the best ones. Lenny questions the need for another pair since he already has purple ones, but Bob points out he has multiple black pairs. Lenny then decides to choose the best quality among the first or third pair, and thanks Bob for the help.\n",
      "\n",
      "Okay, so I need to write a concise summary of this conversation. Let me read through it again to make sure I\n",
      "\n",
      "Fine-tuned Model Summary: Lenny asks Bob for advice on choosing between purple and black trousers. Bob suggests choosing based on quality and outfit options. Lenny decides to buy the best quality pair.\n",
      "\n",
      "Okay, so I need to summarize this conversation between Lenny and Bob. Let me read through it again to make sure I understand what's going on.\n",
      "\n",
      "Lenny starts by asking Bob for help with something. Bob responds positively and asks what's up. Lenny then says, \"Which one should I pick?\" and sends three\n",
      "\n",
      "ROUGE Scores (LightEval):\n",
      "Base Model - ROUGE-1: 0.2414, ROUGE-2: 0.0351, ROUGE-L: 0.1897\n",
      "Fine-tuned - ROUGE-1: 0.3186, ROUGE-2: 0.0360, ROUGE-L: 0.1947\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display example predictions from both models\n",
    "num_examples = min(3, len(dataset))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Dialogue (truncated): {dataset[i]['dialogue'][:150]}...\")\n",
    "    print(f\"\\nReference Summary: {dataset[i]['summary']}\")\n",
    "    \n",
    "    # Show predictions from both models\n",
    "    print(f\"\\nBase Model Summary: {base_model_results['predictions'][i]}\")\n",
    "    print(f\"\\nFine-tuned Model Summary: {finetuned_model_results['predictions'][i]}\")\n",
    "    \n",
    "    # Calculate ROUGE scores for this example using LightEval\n",
    "    base_rouge = rouge_metrics.compute(golds=[dataset[i]['summary']], predictions=[base_model_results['predictions'][i]])\n",
    "    finetuned_rouge = rouge_metrics.compute(golds=[dataset[i]['summary']], predictions=[finetuned_model_results['predictions'][i]])\n",
    "    \n",
    "    print(\"\\nROUGE Scores (LightEval):\")\n",
    "    print(f\"Base Model - ROUGE-1: {base_rouge['rouge1']:.4f}, ROUGE-2: {base_rouge['rouge2']:.4f}, ROUGE-L: {base_rouge['rougeL']:.4f}\")\n",
    "    print(f\"Fine-tuned - ROUGE-1: {finetuned_rouge['rouge1']:.4f}, ROUGE-2: {finetuned_rouge['rouge2']:.4f}, ROUGE-L: {finetuned_rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
