{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Retrieval-Augmented Generation (RAG) pipelines with Amazon OpenSearch, Amazon Sagemaker AI, AWS Bedrock, Ragas and Langfuse\n",
    "\n",
    "In this notebook we'll explore ways to evaluate the quality of Retrieval-Augmented Generation (RAG) pipelines with the opensource tools like [RAGAS](https://docs.ragas.io/en/v0.1.21/index.html) and leverage the features in [Langfuse](https://langfuse.com/) to manage and trace the RAG pipelines with traces and spans. We will create a OpenSearch Vector Database and the RAG results generation to show offline evaluation and scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "> If you haven't selected the kernel, please click on the \"Select Kernel\" button at the upper right corner, select Python Environments and choose \".venv (Python 3.9.20) .venv/bin/python Recommended\".\n",
    "\n",
    "> To execute each notebook cell, press Shift + Enter.\n",
    "\n",
    "> ℹ️ You can **skip these prerequisite steps** if you're in an instructor-led workshop using temporary accounts provided by AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to install dependencies if you are not using AWS workshop environment\n",
    "%pip install langfuse datasets ragas python-dotenv sagemaker langchain-aws opensearch-py requests_aws4auth boto3 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you have completed the prerequisites to setup the Langfuse project and API keys in the .env file to connect to self-hosted or cloud Langfuse environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you already define the environment variables in the .env of the vscode server, please skip the following cell\n",
    "# Define the environment variables for langfuse\n",
    "# You can find those values when you create the API key in Langfuse\n",
    "import os\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-e409877e-57f9-4c26-bc63-8bb5fb119b10\" # Your Langfuse project secret key\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-0d278d2c-3be0-4932-b63e-0c2114064d82\" # Your Langfuse project public key\n",
    "os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # Langfuse domain\n",
    "\n",
    "# Required Langfuse environment variables\n",
    "required_env_vars = [\n",
    "    \"LANGFUSE_SECRET_KEY\",\n",
    "    \"LANGFUSE_PUBLIC_KEY\",\n",
    "    \"LANGFUSE_HOST\"\n",
    "]\n",
    "\n",
    "for var in required_env_vars:\n",
    "    assert os.environ.get(var), f\"❌ Environment variable '{var}' is not set!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Langfuse documentation](https://langfuse.com/docs/get-started) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Run the following cells to initialize common libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import pandas as pd  # For working with tabular data\n",
    "import boto3, uuid\n",
    "from botocore.response import StreamingBody\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from datasets import Dataset\n",
    "from random import sample\n",
    "from asyncio import run\n",
    "\n",
    "\n",
    "\n",
    "# Langchain\n",
    "from langchain_aws.chat_models.sagemaker_endpoint import ChatSagemakerEndpoint, ChatModelContentHandler\n",
    "from langchain_core.messages import HumanMessage, AIMessageChunk, SystemMessage\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "\n",
    "# Langfuse\n",
    "import langfuse  # assuming you're using the SDK\n",
    "from langfuse import Langfuse\n",
    "from langfuse.api.resources.commons.types.trace_with_details import TraceWithDetails\n",
    "from langfuse.decorators import observe, langfuse_context\n",
    "\n",
    "\n",
    "# Sagemaker\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# RAGAS\n",
    "import ragas\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize AWS Bedrock clients and check models available in your account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3  # General Python SDK for AWS (including Bedrock)\n",
    "\n",
    "# used to access Bedrock configuration\n",
    "bedrock = boto3.client(service_name=\"bedrock\", region_name=\"us-east-1\")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name=\"bedrock-agent-runtime\", region_name=\"us-east-1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Qwen2.5-1.5B-Instruct Model to Sagemaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"3.0.1\"\n",
    ")\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "hub = {\n",
    "    'HF_TASK': 'text-generation', \n",
    "    'HF_MODEL_ID': 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "}\n",
    "\n",
    "model_for_deployment = HuggingFaceModel(\n",
    "    #model_data=s3_location,\n",
    "    role=role,\n",
    "    env=hub,\n",
    "    image_uri=llm_image,\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(\"qwen25\")\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "model_for_deployment.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    routing_config = {\n",
    "        \"RoutingStrategy\":  sagemaker.enums.RoutingStrategy.LEAST_OUTSTANDING_REQUESTS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Langfuse client and check credentials are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langfuse client\n",
    "langfuse = Langfuse()\n",
    "if langfuse.auth_check():\n",
    "    print(\"Langfuse has been set up correctly\")\n",
    "    print(f\"You can access your Langfuse instance at: {os.environ['LANGFUSE_HOST']}\")\n",
    "else:\n",
    "    print(\n",
    "        \"Credentials not found or invalid. Check your Langfuse API key and host in the .env file.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Open Search Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Update your IAM role:\n",
    "\n",
    "- Add the following policy to your IAM role:\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"aoss:*\",\n",
    "                \"es:*\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Create the OpenSearch Domain (Once)\n",
    "\n",
    "- From the AWS Console:\n",
    "  - Go to OpenSearch Service\n",
    "  - Click Create domain\n",
    "  - Select Deployment type: \"Development and testing\"\n",
    "  - **Choose:**\n",
    "   - Domain name: ragas-langfuse\n",
    "   - Engine version: latest stable (e.g., 2.11+)\n",
    "   - In Data nodes, keep defaults (t3.small.search, 10 GB, 1 AZ)\n",
    "  - **In Network** , choose:\n",
    "    - Public access (if testing from your SageMaker notebook or local machine)\n",
    "  - **In Access policy**, allow:\n",
    "      - Your SageMaker IAM role, or\n",
    "\n",
    "* if you're testing quickly (you can lock it down later)\n",
    "\n",
    "⏳ It takes ~10 mins to spin up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Index your data into the Open Search Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"\"\n",
    "domain = \"\"\n",
    "\n",
    "assert region != \"\", \"Please include your region of choice\"\n",
    "assert domain != \"\", \"Please copy the domain name from the Open Search Console (IPV4) here WITHOUT the https://\"\n",
    "assert domain.find(\"https://\") < 0, \"Please remove the https:// and make sure it is the IPV4 domain\"\n",
    "\n",
    "\n",
    "# Setup OpenSearch client\n",
    "index_name = \"documents\"\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    \"es\",\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "opensearch = OpenSearch(\n",
    "    hosts=[{'host': domain, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection  # ✅ this makes AWS4Auth compatible\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest a doc\n",
    "def index_document(content, file_name):\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    doc = {\n",
    "        \"file_name\": file_name,\n",
    "        \"content\": content\n",
    "    }\n",
    "    response = opensearch.index(index=index_name, id=doc_id, body=doc)\n",
    "    print(\"✅ Indexed:\", response)\n",
    "\n",
    "# Index all files\n",
    "\n",
    "corpus_dir = \"./datasets/corpus\"\n",
    "\n",
    "for fname in os.listdir(corpus_dir):\n",
    "    if fname.endswith(\".txt\"):\n",
    "        file_path = os.path.join(corpus_dir, fname)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        print(f\"📥 Indexing: {fname}\")\n",
    "        index_document(content, fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Test your Open Search Domain and retreive a sample result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query_text, index_name=\"documents\", size=5):\n",
    "    response = opensearch.search(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"content\": {\n",
    "                        \"query\": query_text\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        size=size\n",
    "    )\n",
    "    \n",
    "    hits = response['hits']['hits']\n",
    "    print(f\"\\n🔍 Found {len(hits)} result(s):\\n\")\n",
    "    for hit in hits:\n",
    "        print(f\"📄 {hit['_source']['file_name']}\")\n",
    "        print(f\"🧠 Score: {hit['_score']}\")\n",
    "        snippet = hit['_source']['content']\n",
    "        print(f\"📝 Content: {snippet}...\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_documents(\"Bretton Woods Accord and price of gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📂 Test Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "fiqa_eval = load_dataset(\"explodinggradients/fiqa\", \"ragas_eval\")[\"baseline\"]\n",
    "fiqa_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 RAGAS Evaluation Metrics\n",
    "\n",
    "We're going to measure the following aspects of a RAG system. These metrics are defined in **[RAGAS]**(https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/):\n",
    "\n",
    "- 🔍 **[Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/)**  \n",
    "  Measures how factually consistent the generated answer is with the retrieved context. It evaluates whether the answer could reasonably be derived from the context.\n",
    "\n",
    "- 🎯 **[Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/)**  \n",
    "  Assesses how relevant the generated answer is to the original user query. A high score indicates the answer is on-topic and useful.\n",
    "\n",
    "- 🧠 **[Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/)**  \n",
    "  Measures how many of the retrieved contexts are truly relevant to answering the question. Precision reflects the \"purity\" of the retrieved chunks.\n",
    "\n",
    "- 📥 **[Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_recall/)**  \n",
    "  Evaluates how well the retrieved context covers the information needed to answer the question completely. High recall means fewer relevant facts are missed.\n",
    "\n",
    "- 🧬 **[Answer Similarity](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_similarity/)**  \n",
    "  Compares the generated answer to a reference answer (if available), measuring how semantically close they are using embedding-based similarity.\n",
    "\n",
    "- ✅ **[Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_correctness/)**  \n",
    "  Evaluates whether the generated answer is factually correct and aligns with known ground-truth answers, if such references are available.\n",
    "\n",
    "> 📚 Want to dive deeper into how each metric is computed?  \n",
    "Check out the full [RAGAS metrics documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "metrics=[\n",
    "        ragas.metrics.answer_relevancy,\n",
    "        ragas.metrics.faithfulness,\n",
    "        ragas.metrics.context_precision,\n",
    "        ragas.metrics.context_recall,\n",
    "        ragas.metrics.answer_similarity,\n",
    "        ragas.metrics.answer_correctness,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            print(metric.name + \" llm\")\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            print(metric.name + \" embedding\")\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to initialize the metrics with LLMs and embedding models of your choice. In this example we are going to use the Qwen2.5-1.5B-Instruct model and amazon.titan-embed-text-v1 embedding model, and use the convenience wrappers from the `langchain-aws` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Sagemaker Chat Wrapper for the Qwen2.5 1.5B Instruct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.Session().client('sagemaker-runtime')\n",
    "endpoint_name = \"SET ENDPOINT NAME\"\n",
    "assert endpoint_name != \"SET ENDPOINT NAME\", f\"❌ endpoint name is not set!\"\n",
    "\n",
    "class ContentHandler(ChatModelContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt, model_kwargs: Dict) -> bytes:\n",
    "        body = {\n",
    "            \"messages\": prompt,\n",
    "            \"stream\": True,\n",
    "            **model_kwargs  # Ensure all model parameters are passed\n",
    "        }\n",
    "        return json.dumps(body).encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: StreamingBody) -> AIMessageChunk:\n",
    "        stop_token = \"[DONE]\"\n",
    "        try:\n",
    "            all_content = []\n",
    "\n",
    "            # Process streaming response line by line\n",
    "            for line in output.iter_lines():\n",
    "                if line:\n",
    "                    line = line.decode(\"utf-8\").strip()\n",
    "\n",
    "                    # Skip empty lines or lines without \"data:\"\n",
    "                    if not line.startswith(\"data:\"):\n",
    "                        continue\n",
    "\n",
    "                    # Validate and parse JSON\n",
    "                    try:\n",
    "                        json_data = json.loads(line[6:])\n",
    "                        \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        #print(f\"Skipping invalid JSON chunk: {line}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for stop token\n",
    "                    if json_data.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\") == stop_token:\n",
    "                        break\n",
    "                    \n",
    "                    # Extract content and append to the list\n",
    "                    content = json_data[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                    all_content.append(content)\n",
    "\n",
    "            # Join all chunks into a single string\n",
    "            full_response = \"\".join(all_content)\n",
    "            return AIMessageChunk(content=full_response)\n",
    "        except Exception as e:\n",
    "            return AIMessageChunk(content=f\"Error processing response: {str(e)}\")\n",
    "\n",
    "\n",
    "chat_content_handler = ContentHandler()\n",
    "\n",
    "chat_llm = ChatSagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    client=sm,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,  # Adjust temperature for balanced randomness\n",
    "        \"max_new_tokens\": 1200,  # Ensure sufficient token generation\n",
    "        \"top_p\": 0.95,  # Use nucleus sampling for diversity\n",
    "        \"do_sample\": True  # Enable sampling for generative tasks\n",
    "    },\n",
    "    content_handler=chat_content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score with RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatSagemakerEndpoint(\n",
    "    name=\"Testmodel\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    client=sm,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,  # Adjust temperature for balanced randomness\n",
    "        \"max_new_tokens\": 1200,  # Ensure sufficient token generation\n",
    "        \"top_p\": 0.95,  # Use nucleus sampling for diversity\n",
    "        \"do_sample\": True  # Enable sampling for generative tasks\n",
    "    },\n",
    "    content_handler=chat_content_handler\n",
    ")\n",
    "\n",
    "# Use correct region for Titan Embed (e.g., us-east-1)\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",  # Titan Embed supported here\n",
    ")\n",
    "\n",
    "emb = BedrockEmbeddings(\n",
    "    client=client,\n",
    "    model_id=\"amazon.titan-embed-text-v1\",  # Case-sensitive!\n",
    ")\n",
    "\n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=LangchainLLMWrapper(llm),\n",
    "    embedding=LangchainEmbeddingsWrapper(emb),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace eval results with Langfuse\n",
    "\n",
    "You can use model-based evaluation with Ragas in 2 ways:\n",
    "1. Score every trace: This means you will run the evaluations for each trace item. This gives you much better idea of how each call made to your RAG pipelines is performing, but please be mindful of the cost.\n",
    "\n",
    "2. Score with sampling: In this method we will take random samples of traces on a periodic basis and score them. This brings down the cost and gives you a rough estimate the performance of your app but may miss out on important samples.\n",
    "\n",
    "In this example, we will demonstrate both solutions using prebuilt dataset and a live RAG pipeline with AWS Open Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score every trace\n",
    "\n",
    "Lets take a small example of a single trace and see how you can score that with Ragas. We first define a utility function to score your trace with the metrics you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def score_with_ragas(query, chunks, answer, metrics):\n",
    "    scores = {}\n",
    "    for metric in metrics:\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=query,\n",
    "            retrieved_contexts=chunks,\n",
    "            response=answer,\n",
    "            reference=chunks[0]\n",
    "        )\n",
    "        print(f\"calculating {metric.name}\")\n",
    "        scores[metric.name] = await metric.single_turn_ascore(sample)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring sample dataset item\n",
    "\n",
    "You compute the score with each request. Below we will go through a dummy application that does the following steps:\n",
    "\n",
    "- Gets a question from the user\n",
    "- Fetch context from the database or vector store that can be used to answer the question from the user\n",
    "- Pass the question and the contexts to the LLM to generate the answer\n",
    "\n",
    "In this case we are demonstrating the use of the Langfuse Python [low-level SDK](https://langfuse.com/docs/sdk/python/low-level-sdk) to log the traces with more granular controls. You can also see an example with the [decorator](https://langfuse.com/docs/sdk/python/decorators) in the later section or read more about them the [langfuse documentation](https://langfuse.com/docs/sdk/overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new trace when you get a question\n",
    "row = fiqa_eval[0]\n",
    "question = row[\"question\"]\n",
    "trace = langfuse.trace(name=\"rag-fiqa\")\n",
    "\n",
    "# retrieve the relevant chunks\n",
    "# chunks = get_similar_chunks(question)\n",
    "contexts = row[\"contexts\"]\n",
    "# pass it as span\n",
    "trace.span(\n",
    "    name=\"retrieval\", input={\"question\": question}, output={\"contexts\": contexts}\n",
    ")\n",
    "\n",
    "# use llm to generate a answer with the chunks\n",
    "# answer = get_response_from_llm(question, chunks)\n",
    "answer = row[\"answer\"]\n",
    "trace.generation(\n",
    "    name=\"generation\",\n",
    "    input={\"question\": question, \"contexts\": contexts},\n",
    "    output={\"answer\": answer},\n",
    ")\n",
    "\n",
    "# compute scores for the question, context, answer tuple\n",
    "ragas_scores = await score_with_ragas(question, contexts, answer, metrics)\n",
    "ragas_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Now you can see this is traced in langfuse but with no score attached, we can check it in the Langfuse UI at:\\n{os.environ['LANGFUSE_HOST']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then attach the scores to the trace by running the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the scores\n",
    "for m in metrics:\n",
    "    trace.score(name=m.name, value=ragas_scores[m.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the score is attached. It should look similar to this\n",
    "\n",
    "![](images/sagemaker_langfuse_score_single.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring RAG\n",
    "We have already setup the Open Search Database in the first section, we can now **evaluate** the quality of its results against a test dataset - to help us **optimize** the configuration for high quality and low cost.\n",
    "\n",
    "First, let's load the sample dataset of questions, reference answers, and their source documents (to find more of how to prepare this dataset, please see more details in [this github](https://github.com/aws-samples/llm-evaluation-methodology/blob/main/datasets/Prepare-SQuAD.ipynb)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_df = pd.read_json(\"datasets/qa.manifest.jsonl\", lines=True)\n",
    "dataset_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Records in this dataset include:\n",
    "\n",
    "- (`doc`) The full text of the source document for this example\n",
    "- (`doc_id`) A unique identifier for the source document\n",
    "- (`question`) The user question to be asked\n",
    "- (`question_id`) A unique identifier for the question\n",
    "- (`answers`) A list of (possibly multiple) reference 'correct' answers, supported by the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in [Ragas' API Reference](https://docs.ragas.io/en/latest/references/evaluation.html), records in Ragas evaluation datasets typically include:\n",
    "\n",
    "- The `question` that was asked\n",
    "- The `answer` the system generated\n",
    "- The actual text `contexts` the answer was based on (i.e. snippets of document text retrieved by the search engine)\n",
    "- The `ground_truth` answer(s)\n",
    "\n",
    "Here we will integrate [Langfuse Tracking](https://langfuse.com/docs/tracing) into the RAG pipeline with the Langfuse Python SDK using the `@observe()` decorator.\n",
    "\n",
    "We can run an example question through the OpenSearch Vector database to retrieve and generate pipeline as shown below, and extract the references ready to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bedrock Runtime\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "@observe(name=\"OpenSearch RAG with Qwen\")\n",
    "def retrieve_and_generate(\n",
    "    question: str,\n",
    "    top_k: int = 3,\n",
    "    system_prompt: str = \"You are a helpful assistant. Use the context to answer concisely.\",\n",
    "    **kwargs,\n",
    "):\n",
    "    # Step 1: Retrieve relevant context from OpenSearch\n",
    "    response = opensearch.search(\n",
    "        index=\"documents\",\n",
    "        body={\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"content\": {\n",
    "                        \"query\": question\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        size=top_k\n",
    "    )\n",
    "    \n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    contexts = [hit[\"_source\"][\"content\"] for hit in hits]\n",
    "    doc_ids = [hit[\"_id\"] for hit in hits]\n",
    "\n",
    "    # Step 2: Format prompt with retrieved context\n",
    "    combined_context = \"\\n\\n\".join(contexts)\n",
    "    full_prompt = f\"\"\"Context:\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 3: Call your SageMaker-hosted model using LangChain\n",
    "    messages: List = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=full_prompt)\n",
    "    ]\n",
    "    \n",
    "    response_chunk = chat_llm.invoke(messages)\n",
    "    answer = response_chunk.content  # already joined by your handler\n",
    "\n",
    "    # Step 4: Log trace to Langfuse\n",
    "    langfuse_context.update_current_observation(\n",
    "        input={\"question\": question, \"contexts\": contexts},\n",
    "        output=answer,\n",
    "        model=endpoint_name,\n",
    "        session_id=\"opensearch-rag-session\",\n",
    "        tags=[\"dev\", \"qwen\", \"opensearch\"],\n",
    "        metadata=kwargs,\n",
    "    )\n",
    "\n",
    "    trace_id = langfuse_context.get_current_trace_id()\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_doc_ids\": doc_ids,\n",
    "        \"retrieved_doc_texts\": contexts[:300],\n",
    "        \"trace_id\": trace_id,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run RAG as requests come in and score the results immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import run\n",
    "\n",
    "\n",
    "langfuse_client = Langfuse()  # picks up env vars: LANGFUSE_PUBLIC_KEY, SECRET_KEY, HOST\n",
    "\n",
    "\n",
    "@observe(name=\"OpenSearch, Qwen2.5, Langfuse Pipeline\")\n",
    "def rag_pipeline(\n",
    "    question: str,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    metrics: Optional[Any] = None,\n",
    "):\n",
    "    generated_answer = retrieve_and_generate(\n",
    "        question=question,\n",
    "        top_k=3,  # or whatever makes sense for your context window\n",
    "        system_prompt=\"You are a helpful assistant. Use the context below to answer the question.\"\n",
    "    )\n",
    "\n",
    "    answer = generated_answer[\"answer\"]\n",
    "    contexts = generated_answer[\"retrieved_doc_texts\"]\n",
    "    trace_id = generated_answer[\"trace_id\"]\n",
    "\n",
    "    \n",
    "    metrics=[\n",
    "            # A looot of metrics to give a general overview:\n",
    "            ragas.metrics.answer_relevancy,\n",
    "            ragas.metrics.faithfulness,\n",
    "            ragas.metrics.context_precision,\n",
    "            ragas.metrics.context_recall,\n",
    "            ragas.metrics.answer_similarity,\n",
    "            ragas.metrics.answer_correctness,\n",
    "        ]\n",
    "\n",
    "\n",
    "    score = run(score_with_ragas(question, contexts, answer=answer, metrics=metrics))\n",
    "\n",
    "    langfuse_context.update_current_trace(\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        tags=[\"dev\", \"opensearch\", \"qwen\"]\n",
    "    )\n",
    "\n",
    "    for s in score:\n",
    "        langfuse_client.score(name=s, value=score[s])\n",
    "\n",
    "\n",
    "    print(f\"🔗 Langfuse trace: https://cloud.langfuse.com/trace/{trace_id}\")\n",
    "\n",
    "    return generated_answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_pipeline(\n",
    "    question=dataset_df.iloc[0][\"question\"],\n",
    "    user_id=\"AWSome\",\n",
    "    session_id=\"qwen-test-session\"\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring with sampling\n",
    "\n",
    "Scoring every production trace can be time-consuming and costly depending on your application architecture and traffic. In that case, it's better to start off with a sampling method. Decide a timespan you want to run the batch process and the number of traces you want to sample from that time slice. Create a dataset and call ragas.evaluate to analyze the result.\n",
    "\n",
    "You can run this periodically to keep track of how the scores are changing across timeslices and figure out if there are any discrepancies.\n",
    "\n",
    "We will evaluate the existing results generated previously by the `retrieve_and_generate()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate 10 production traces by running RAG on the first 10 questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_generated_outputs = [\n",
    "    retrieve_and_generate(\n",
    "        question=rec[\"question\"],\n",
    "        top_k=3,  # or whatever makes sense for your context window\n",
    "        system_prompt=\"You are a helpful assistant. Use the context below to answer the question.\"\n",
    "    )\n",
    "    for _, rec in dataset_df.head(10).iterrows()\n",
    "]\n",
    "rag_generated_outputs[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the results are uploaded to langfuse you can retrieve it as needed with this handy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces(\n",
    "    limit: int = 5,\n",
    "    name: Optional[str] = None,\n",
    "    user_id: Optional[str] = None,\n",
    "    session_id: Optional[str] = None,\n",
    "    from_timestamp: Optional[str] = None,\n",
    "    to_timestamp: Optional[str] = None,\n",
    ") -> List[TraceWithDetails]:\n",
    "    \"\"\"Query Langfuse for traces matching the given filters.\n",
    "    See https://langfuse.com/docs/query-traces for more details.\"\"\"\n",
    "\n",
    "    all_data = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        response = langfuse_client.fetch_traces(\n",
    "            page=page,\n",
    "            name=name,\n",
    "            user_id=user_id,\n",
    "            session_id=session_id,\n",
    "            from_timestamp=from_timestamp,\n",
    "            to_timestamp=to_timestamp,\n",
    "        )\n",
    "        if not response.data:\n",
    "            break\n",
    "        page += 1\n",
    "        all_data.extend(response.data)\n",
    "        if len(all_data) > limit:\n",
    "            break\n",
    "\n",
    "    return all_data[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRACES_TO_SAMPLE = 3\n",
    "traces = get_traces(name=\"OpenSearch RAG with Qwen\", limit=10)\n",
    "if len(traces) > NUM_TRACES_TO_SAMPLE:\n",
    "    traces_sample = sample(traces, NUM_TRACES_TO_SAMPLE)\n",
    "else:\n",
    "    traces_sample = traces\n",
    "\n",
    "print(f\"Sampled {len(traces_sample)} traces from {len(traces)} filtered traces\")\n",
    "for trace in traces_sample:\n",
    "    print(f\"Trace ID: {trace.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets make a batch and score it. Ragas uses huggingface dataset object to build the dataset and run the evaluation. If you run this on your own production data, use the right keys to extract the question, contexts and answer from the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on a sample\n",
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "    \"trace_id\": [],\n",
    "}\n",
    "\n",
    "for sample in traces_sample:\n",
    "    evaluation_batch[\"question\"].append(sample.input[\"question\"])\n",
    "    evaluation_batch[\"contexts\"].append(sample.input[\"contexts\"])\n",
    "    evaluation_batch[\"answer\"].append(sample.output)\n",
    "    evaluation_batch[\"trace_id\"].append(sample.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ragas evaluate function to score an entire dataset instead of single turn. See [Ragas evaluate](https://docs.ragas.io/en/latest/references/evaluate/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(evaluation_batch)\n",
    "evals_results = evaluate(\n",
    "    ds,\n",
    "    llm=llm,\n",
    "    embeddings=emb,\n",
    "    metrics=[Faithfulness(), ResponseRelevancy()],\n",
    ")\n",
    "evals_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! You can see the scores over a time period. Let's render the results in a dataframe to see the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evals_results.to_pandas()\n",
    "\n",
    "# add the langfuse trace_id to the result dataframe\n",
    "df[\"trace_id\"] = ds[\"trace_id\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push the scores back into Langfuse and attach them to the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in df.iterrows():\n",
    "    for metric_name in [\"faithfulness\", \"answer_relevancy\"]:\n",
    "        langfuse.score(\n",
    "            name=metric_name, value=row[metric_name], trace_id=row[\"trace_id\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now go back to the Langfuse console and check the updated scores in the traces.\n",
    "\n",
    "![](images/score-with-sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratuations!\n",
    "You have now learnt how to use AWS OpenSearch, Amazon Sagemaker AI , AWS Bedrock and Langfuse to evaluate and score RAG workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
